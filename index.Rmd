--- 
title: "NBA All-Star Predictions"
author: "Eric Drew"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
---

# NBA All-Star Predictions
## Introduction
**Objective**

Utilize an Extreme Gradient Boosting model, hereafter referred to as "xgboost", to predict the NBA All-Star roster for the 2022-23 season.

**Background** 

Each season in February, 24 of the NBA's top performing players are selected as All-Stars. 12 players represent each of the East and West Conferences with the following breakdown:



*   Four guards (Point Guard, Shooting Guard)
*   Six frontcourt players (Small Forward, Power Forward, Center)
*   Two additional players regardless of position

Starting lineups are selected by a combination of fan, current player, and media votes, while the reserve players are selected by the league's 30 head coaches. Injured players will be replaced by a player selected by the league's Commissioner, Adam Silver [1].

**Data**

The model will be built off of player's "per game" statistics from the 2003-04 to 2021-22 seasons, accessed from the public sports database Basketball-Reference [2]. Player statistics from the current season, which is only ~30 games in at the time of writing, will be used as our test dataset. My goal is to update my predictions in January and again in February just before the rosters are announced, allowing me to assess my models predictive power in real-time. While the model will not change during this time period, new data will be available as players continue to complete more games in the coming months.  

## Methodology/Data Prep
I will be utilizing a combination of common and advanced Python libraries to complete the project. The following code is living and may be adapted as the project continues. 

```{r terminal, echo=F}
# RUN ME TO GET BACK TO AN R TERMINAL
```

```{python}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import time
import sklearn
import seaborn as sns
import xgboost as xgb
from scipy.stats import zscore
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from sklearn.compose import make_column_transformer
from sklearn.model_selection import GridSearchCV, KFold, RepeatedStratifiedKFold, train_test_split, cross_val_score, RandomizedSearchCV
from sklearn.metrics import roc_curve, auc, roc_auc_score, RocCurveDisplay, plot_confusion_matrix
from sklearn.calibration import calibration_curve
```
Now we will read in our 19 seasons of player per game statistics to build our training set on. 

```{python}
s03 = pd.read_csv('C:/Users/ericd/OneDrive - North Carolina State University/Desktop/NBA-AllStar-Predictions/Data/season03-04.csv')
s04 = pd.read_csv('C:/Users/ericd/OneDrive - North Carolina State University/Desktop/NBA-AllStar-Predictions/Data/season04-05.csv')
s05 = pd.read_csv('C:/Users/ericd/OneDrive - North Carolina State University/Desktop/NBA-AllStar-Predictions/Data/season05-06.csv')
s06 = pd.read_csv('C:/Users/ericd/OneDrive - North Carolina State University/Desktop/NBA-AllStar-Predictions/Data/season06-07.csv')
s07 = pd.read_csv('C:/Users/ericd/OneDrive - North Carolina State University/Desktop/NBA-AllStar-Predictions/Data/season07-08.csv')
s08 = pd.read_csv('C:/Users/ericd/OneDrive - North Carolina State University/Desktop/NBA-AllStar-Predictions/Data/season08-09.csv')
s09 = pd.read_csv('C:/Users/ericd/OneDrive - North Carolina State University/Desktop/NBA-AllStar-Predictions/Data/season09-10.csv')
s10 = pd.read_csv('C:/Users/ericd/OneDrive - North Carolina State University/Desktop/NBA-AllStar-Predictions/Data/season10-11.csv')
s11 = pd.read_csv('C:/Users/ericd/OneDrive - North Carolina State University/Desktop/NBA-AllStar-Predictions/Data/season11-12.csv')
s12 = pd.read_csv('C:/Users/ericd/OneDrive - North Carolina State University/Desktop/NBA-AllStar-Predictions/Data/season12-13.csv')
s13 = pd.read_csv('C:/Users/ericd/OneDrive - North Carolina State University/Desktop/NBA-AllStar-Predictions/Data/season13-14.csv')
s14 = pd.read_csv('C:/Users/ericd/OneDrive - North Carolina State University/Desktop/NBA-AllStar-Predictions/Data/season14-15.csv')
s15 = pd.read_csv('C:/Users/ericd/OneDrive - North Carolina State University/Desktop/NBA-AllStar-Predictions/Data/season15-16.csv')
s16 = pd.read_csv('C:/Users/ericd/OneDrive - North Carolina State University/Desktop/NBA-AllStar-Predictions/Data/season16-17.csv')
s17 = pd.read_csv('C:/Users/ericd/OneDrive - North Carolina State University/Desktop/NBA-AllStar-Predictions/Data/season17-18.csv')
s18 = pd.read_csv('C:/Users/ericd/OneDrive - North Carolina State University/Desktop/NBA-AllStar-Predictions/Data/season18-19.csv')
s19 = pd.read_csv('C:/Users/ericd/OneDrive - North Carolina State University/Desktop/NBA-AllStar-Predictions/Data/season19-20.csv')
s20 = pd.read_csv('C:/Users/ericd/OneDrive - North Carolina State University/Desktop/NBA-AllStar-Predictions/Data/season20-21.csv')
s21 = pd.read_csv('C:/Users/ericd/OneDrive - North Carolina State University/Desktop/NBA-AllStar-Predictions/Data/season21-22.csv')
```


There were a handful of things that needed to be done to the datasets before I could concatenate all 19 seasons into the cumulative training set.

**Scaling by Season**

The NBA is an incredibly dynamic league and has changed greatly over the last decades. For example, the average points scored by a team per game has fluctuated a good bit just in the time horizon we are concerned with in this project, with a high of 112.1 PPG in 2020-21 and low of 93.4 PPG in 2003-04, a difference of nearly 20 points per game [3]. Because of changes like these, it is difficult to compare raw statistics between players across seasons, especially those over a decade apart. For example, a player scoring 20 PPG in 2003 was likely more impactful than a 20 PPG scorer in 2020, and in the context of our project's scope, may be the difference between an All-Star or not. The question of whether a player is an All-Star is likely not as simple as "how many points per game do they score?", but "how does their scoring average compare to the rest of the league?"


To combat this challenge, I scaled each season's data individually using Z-scores, allowing us to measure how strong a player's stat line was compared to the league *that season*. We then will be able to compare players across time by how much they stood out across a distribution, regardless of the season they played in. 

Examples of these calculations are shown below.

```
df['PTS'] = zscore(df['PTS'])
df['Age'] = zscore(df['Age'])`
df['PF'] = zscore(df['PF'])
```

**Observation Exclusion**

In exploring the data, one challenge I discovered in the data was that for players who were traded or played for multiple seasons had more than one observation. These players had observations representing their stats while playing for each team during a season, as well as a cumulative season stat line, which was represented by their team being "TOT" for total. For the purposes of my model, I dropped the individual team stat line observations, and kept only the total season stats for a player.

```
df = df.sort_values(['Player','G'], ascending = [True, False])
df = df.drop_duplicates(subset=['Player'], keep='first')
```

Another concern was the data being too imbalanced for a model to find meaningful signal in. Most NBA seasons see over 450 individual players log minutes in at least one game. Left as this, the league would contain roughly ~5% All-Stars, crossing into rare-event territory. Seeing as players who rarely see the court are not going to be in contention for making the All-Star roster as is, I decided to subset our data to retain only players who played *at least* 15 minutes per 48 minute game. This was in an attempt to reduce the noise caused by an overwhelming amount of players who only enter games for a handful of minutes per game at best, who's statistics would intuitively not bring any predictive power in separating All-Stars versus those who are not. 

```
df = df[df['MP'] >= 15]
```

Lastly, there were some miscellaneous manipulations that needed to be made to the datasets, such  dropping irrelevant variables such as an ID variable, and removing random special characters found in some observations. 

```
df = df.drop(['Rk','Player-additional','Tm'], axis=1)
df['Player'] = df['Player'].str.replace('\W', '', regex=True)
```
To more efficiently apply these manipulations across all 19 datasets, I created a function to run them each through, as opposed to writing these commands for each one. The code for this, which is a culmination of the above commands, is shown below. 

```{python}
#create list of dataframes to be used in loops(TRAIN ONLY)
dfList = [s03,s04,s05,s06,s07,s08,s09,s10,s11,s12,s13,s14,s15,s16,s17,s18,s19,s20,s21]
```

```{python}
#cleaning function removing TOTs, keep to players >=15MPG, scale continuous variables, remove special characters on name
def trainPrep(df):
    df = df.drop(['Rk','Player-additional','Tm'], axis=1)
    df = df.sort_values(['Player','G'], ascending = [True, False])
    df = df.drop_duplicates(subset=['Player'], keep='first')
    df = df[df['MP'] >= 15]
    df['Age'] = zscore(df['Age'])
    df['G'] = zscore(df['G'])
    df['GS'] = zscore(df['GS'])
    df['MP'] = zscore(df['MP'])
    df['FG'] = zscore(df['FG'])
    df['FGA'] = zscore(df['FGA'])
    df['FG%'] = zscore(df['FG%'])
    df['3P'] = zscore(df['3P'])
    df['3PA'] = zscore(df['3PA'])
    df['2P'] = zscore(df['2P'])
    df['2PA'] = zscore(df['2PA'])
    df['2P%'] = zscore(df['2P%'])
    df['eFG%'] = zscore(df['eFG%'])
    df['FT'] = zscore(df['FT'])
    df['FTA'] = zscore(df['FTA'])
    df['FT%'] = zscore(df['FT%'])
    df['ORB'] = zscore(df['ORB'])
    df['DRB'] = zscore(df['DRB'])
    df['TRB'] = zscore(df['TRB'])
    df['AST'] = zscore(df['AST'])
    df['STL'] = zscore(df['STL'])
    df['BLK'] = zscore(df['BLK'])
    df['TOV'] = zscore(df['TOV'])
    df['PF'] = zscore(df['PF'])
    df['PTS'] = zscore(df['PTS'])
    df['Player'] = df['Player'].str.replace('\W', '', regex=True)
    return df
```

```{python}
#Run training sets through cleaning function
s03,s04,s05,s06,s07,s08,s09,s10,s11,s12,s13,s14,s15,s16,s17,s18,s19,s20,s21 =[trainPrep(df) for df in dfList]
```

