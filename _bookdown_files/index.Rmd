--- 
title: "NBA All-Star Predictions"
author: "Eric Drew"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
---

# NBA All-Star Predictions
## Introduction
**Objective**

Utilize an Extreme Gradient Boosting model, hereafter referred to as "XGBoost", to predict the NBA All-Star roster for the 2022-23 season.

**Background** 

Each season in February, 24 of the NBA's top performing players are selected as All-Stars. 12 players represent each of the East and West Conferences with the following breakdown:



*   Four guards (Point Guard, Shooting Guard)
*   Six frontcourt players (Small Forward, Power Forward, Center)
*   Two additional players, regardless of position

Starting lineups are selected by a combination of fan, current player, and media votes, while the reserve players are chosen by the league's 30 head coaches. Injured players will be replaced by a player selected by the league's Commissioner, Adam Silver.

**Data**

The model will be built off of players' "per game" statistics from the 2003-04 to 2021-22 seasons, accessed from the public sports database Basketball-Reference. Player statistics from the current season, which is only ~30 games in at the time of writing, will be used as our test dataset. I aim to update my predictions in January and again in February just before the rosters are announced, allowing me to assess my models predictive power in real-time. While the model will not change during this period, new data will be available as players continue to complete more games in the coming months.  

## Methodology/Data Prep
I will utilize a combination of common and advanced Python libraries to complete the project. The following code is living and may be adapted as the project continues. 

```{r terminal, echo=F}
# RUN ME TO GET BACK TO AN R TERMINAL
```

```{python}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import time
import sklearn
import seaborn as sns
import xgboost as xgb
from scipy.stats import zscore
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from sklearn.compose import make_column_transformer
from sklearn.model_selection import GridSearchCV, KFold, RepeatedStratifiedKFold, train_test_split, cross_val_score, RandomizedSearchCV
from sklearn.metrics import roc_curve, auc, roc_auc_score, RocCurveDisplay, plot_confusion_matrix
from sklearn.calibration import calibration_curve
```
Now we will read in our 19 seasons of player per game statistics to build our training set on. 

```{python}
s03 = pd.read_csv('C:/Users/ericd/OneDrive - North Carolina State University/Desktop/NBA-AllStar-Predictions/Data/season03-04.csv')
s04 = pd.read_csv('C:/Users/ericd/OneDrive - North Carolina State University/Desktop/NBA-AllStar-Predictions/Data/season04-05.csv')
s05 = pd.read_csv('C:/Users/ericd/OneDrive - North Carolina State University/Desktop/NBA-AllStar-Predictions/Data/season05-06.csv')
s06 = pd.read_csv('C:/Users/ericd/OneDrive - North Carolina State University/Desktop/NBA-AllStar-Predictions/Data/season06-07.csv')
s07 = pd.read_csv('C:/Users/ericd/OneDrive - North Carolina State University/Desktop/NBA-AllStar-Predictions/Data/season07-08.csv')
s08 = pd.read_csv('C:/Users/ericd/OneDrive - North Carolina State University/Desktop/NBA-AllStar-Predictions/Data/season08-09.csv')
s09 = pd.read_csv('C:/Users/ericd/OneDrive - North Carolina State University/Desktop/NBA-AllStar-Predictions/Data/season09-10.csv')
s10 = pd.read_csv('C:/Users/ericd/OneDrive - North Carolina State University/Desktop/NBA-AllStar-Predictions/Data/season10-11.csv')
s11 = pd.read_csv('C:/Users/ericd/OneDrive - North Carolina State University/Desktop/NBA-AllStar-Predictions/Data/season11-12.csv')
s12 = pd.read_csv('C:/Users/ericd/OneDrive - North Carolina State University/Desktop/NBA-AllStar-Predictions/Data/season12-13.csv')
s13 = pd.read_csv('C:/Users/ericd/OneDrive - North Carolina State University/Desktop/NBA-AllStar-Predictions/Data/season13-14.csv')
s14 = pd.read_csv('C:/Users/ericd/OneDrive - North Carolina State University/Desktop/NBA-AllStar-Predictions/Data/season14-15.csv')
s15 = pd.read_csv('C:/Users/ericd/OneDrive - North Carolina State University/Desktop/NBA-AllStar-Predictions/Data/season15-16.csv')
s16 = pd.read_csv('C:/Users/ericd/OneDrive - North Carolina State University/Desktop/NBA-AllStar-Predictions/Data/season16-17.csv')
s17 = pd.read_csv('C:/Users/ericd/OneDrive - North Carolina State University/Desktop/NBA-AllStar-Predictions/Data/season17-18.csv')
s18 = pd.read_csv('C:/Users/ericd/OneDrive - North Carolina State University/Desktop/NBA-AllStar-Predictions/Data/season18-19.csv')
s19 = pd.read_csv('C:/Users/ericd/OneDrive - North Carolina State University/Desktop/NBA-AllStar-Predictions/Data/season19-20.csv')
s20 = pd.read_csv('C:/Users/ericd/OneDrive - North Carolina State University/Desktop/NBA-AllStar-Predictions/Data/season20-21.csv')
s21 = pd.read_csv('C:/Users/ericd/OneDrive - North Carolina State University/Desktop/NBA-AllStar-Predictions/Data/season21-22.csv')
```


A handful of things that needed to be done to the datasets before I could concatenate all 19 seasons into the cumulative training set.

**Scaling by Season**

The NBA is an incredibly dynamic league and has changed dramatically over the last decades. For example, the average points scored by a team per game has fluctuated a good bit just in the time horizon we are concerned with in this project, with a high of 112.1 PPG in 2020-21 and a low of 93.4 PPG in 2003-04, a difference of nearly 20 points per game. Because of changes like these, it is difficult to compare raw statistics between players across seasons, especially those over a decade apart. For example, a player scoring 20 PPG in 2003 was likely more impactful than a 20 PPG scorer in 2020, and in the context of our project's scope may be the difference between an All-Star or not. The question of whether a player is an All-Star is likely not as simple as "how many points per game do they score?" but "how does their scoring average compare to the rest of the league?"


To combat this challenge, I scaled each season's data individually using Z-scores, allowing us to measure how strong a player's stat line was compared to the league *that season*. We then will be able to compare players across time by how much they stood out across a distribution, regardless of the season they played in. 

Examples of these calculations are shown below.

```
df['PTS'] = zscore(df['PTS'])
df['Age'] = zscore(df['Age'])`
df['PF'] = zscore(df['PF'])
```

**Observation Exclusion**

In exploring the data, one challenge I discovered in the data was that for players who were traded or played for multiple seasons had more than one observation. These players had observations representing their stats while playing for each team during a season, as well as a cumulative season stat line, which was represented by their team being "TOT" for total. For the purposes of my model, I dropped the individual team stat line observations, and kept only the total season stats for a player.

```
df = df.sort_values(['Player','G'], ascending = [True, False])
df = df.drop_duplicates(subset=['Player'], keep='first')
```

Another concern was the data being too imbalanced for a model to find meaningful signal in. Most NBA seasons see over 450 individual players log minutes in at least one game. Left as this, the league would contain roughly ~5% All-Stars, crossing into rare-event territory. Seeing as players who rarely see the court are not going to be in contention for making the All-Star roster as is, I decided to subset our data to retain only players who played *at least* 15 minutes per 48 minute game. This was in an attempt to reduce the noise caused by an overwhelming amount of players who only enter games for a handful of minutes per game at best, whose statistics would intuitively not bring any predictive power in separating All-Stars versus those who are not. Another point of note
is that across NBA history, the All-Star who played the least minutes per game was Dirk Nowitzki in 2018-19. Since
no other NBA player in history has made an All-Star game without playing at least 15 minutes per game, I deemed
this a useful cutoff for players that would be deemed up for contention to be an All-Star.

```
df = df[df['MP'] >= 15]
```

Lastly, some miscellaneous manipulations needed to be made to the datasets, such  dropping irrelevant variables such as an ID variable, and removing random special characters found in some observations. 

```
df = df.drop(['Rk','Player-additional','Tm'], axis=1)
df['Player'] = df['Player'].str.replace('\W', '', regex=True)
```
To more efficiently apply these manipulations across all 19 datasets, I created a function to run them each through, as opposed to writing these commands for each one. The code for this, which is a culmination of the above commands, is shown below. 

```{python}
#create list of dataframes to be used in loops(TRAIN ONLY)
dfList = [s03,s04,s05,s06,s07,s08,s09,s10,s11,s12,s13,s14,s15,s16,s17,s18,s19,s20,s21]
```

```{python}
#cleaning function removing TOTs, keep to players >=15MPG, scale continuous variables, remove special characters on name
def trainPrep(df):
  df = df.drop(['Rk','Player-additional','Tm'], axis=1)
  df = df.sort_values(['Player','G'], ascending = [True, False])
  df = df.drop_duplicates(subset=['Player'], keep='first')
  df = df[df['MP'] >= 15]
  df['Age'] = zscore(df['Age'])
  df['G'] = zscore(df['G'])
  df['GS'] = zscore(df['GS'])
  df['MP'] = zscore(df['MP'])
  df['FG'] = zscore(df['FG'])
  df['FGA'] = zscore(df['FGA'])
  df['FG%'] = zscore(df['FG%'])
  df['3P'] = zscore(df['3P'])
  df['3PA'] = zscore(df['3PA'])
  df['2P'] = zscore(df['2P'])
  df['2PA'] = zscore(df['2PA'])
  df['2P%'] = zscore(df['2P%'])
  df['eFG%'] = zscore(df['eFG%'])
  df['FT'] = zscore(df['FT'])
  df['FTA'] = zscore(df['FTA'])
  df['FT%'] = zscore(df['FT%'])
  df['ORB'] = zscore(df['ORB'])
  df['DRB'] = zscore(df['DRB'])
  df['TRB'] = zscore(df['TRB'])
  df['AST'] = zscore(df['AST'])
  df['STL'] = zscore(df['STL'])
  df['BLK'] = zscore(df['BLK'])
  df['TOV'] = zscore(df['TOV'])
  df['PF'] = zscore(df['PF'])
  df['PTS'] = zscore(df['PTS'])
  df['Player']=df['Player'].str.replace("\\W",'',regex=True)
  return df
```

```{python}
#Run training sets through cleaning function
s03,s04,s05,s06,s07,s08,s09,s10,s11,s12,s13,s14,s15,s16,s17,s18,s19,s20,s21 =[trainPrep(df) for df in dfList]
```

**Target Variable Creation**

Next, I created lists of each of the All-Star rosters from the past 19 seasons. I then assigned a new variable, 'AllStar', to each player in each dataset. Players on the All-Star roster for that season received a value of 1, while players who were not All-Stars received a 0. An example of this is shown below. The object "ASG03" represents the list of All-Stars from the 2003-04 season.

```{python, echo=F}
ASG03 = ['KobeBryant','KevinGarnett','TimDuncan','SteveFrancis','YaoMing','RayAllen',"ShaquilleONeal",'SamCassell',
         'PejaStojaković','DirkNowitzki','AndreiKirilenko','BradMiller',"JermaineONeal",'AllenIverson','BenWallace',
         'TracyMcGrady','VinceCarter','KenyonMartin','JasonKidd','JamaalMagloire','MettaWorldPeace','BaronDavis',
         'MichaelRedd','PaulPierce']
ASG04 = ['AllenIverson','LeBronJames',"ShaquilleONeal",'VinceCarter','GrantHill','DwyaneWade',"JermaineONeal",
         'ZydrunasIlgauskas','PaulPierce','GilbertArenas','BenWallace','AntawnJamison','KobeBryant','TracyMcGrady',
         'YaoMing','TimDuncan','KevinGarnett','RayAllen', 'ManuGinóbili','DirkNowitzki',"AmareStoudemire",'SteveNash',
         'ShawnMarion', 'RashardLewis']
ASG05 = ['DwyaneWade','LeBronJames','AllenIverson', "ShaquilleONeal",'VinceCarter','BenWallace','RasheedWallace','ChrisBosh',
         'ChaunceyBillups','PaulPierce','RichardHamilton','GilbertArenas','SteveNash','TracyMcGrady','KobeBryant','TimDuncan',
         'YaoMing','TonyParker','RayAllen','ShawnMarion','EltonBrand','KevinGarnett','DirkNowitzki','PauGasol',"JermaineONeal"]
ASG06 = ['KobeBryant','TracyMcGrady','DirkNowitzki','TimDuncan','KevinGarnett','CarmeloAnthony','TonyParker','ShawnMarion',
         'RayAllen',"AmareStoudemire",'JoshHoward','MehmetOkur','LeBronJames','DwyaneWade','ChrisBosh','GilbertArenas',
         "ShaquilleONeal","JermaineONeal",'DwightHoward','JoeJohnson','ChaunceyBillups','VinceCarter','CaronButler','RichardHamilton',
         'JasonKidd','CarlosBoozer','AllenIverson','YaoMing','SteveNash']
ASG07 = ['DwightHoward','LeBronJames','JasonKidd','DwyaneWade','ChrisBosh','RayAllen','ChaunceyBillups','RichardHamilton',
         'RasheedWallace','JoeJohnson','PaulPierce','AntawnJamison','TimDuncan','CarmeloAnthony','AllenIverson','YaoMing',
         'KobeBryant','BrandonRoy','ChrisPaul','DirkNowitzki',"AmareStoudemire",'SteveNash','CarlosBoozer','DavidWest',
         'CaronButler','KevinGarnett']
ASG08 = ['KobeBryant','ChrisPaul',"AmareStoudemire",'TimDuncan','YaoMing','BrandonRoy','TonyParker','ChaunceyBillups',
         'PauGasol','DavidWest','DirkNowitzki',"ShaquilleONeal",'DwightHoward','LeBronJames','DwyaneWade','KevinGarnett',
         'AllenIverson','JoeJohnson','RashardLewis','PaulPierce','RayAllen','DevinHarris','MoWilliams','DannyGranger',
         'ChrisBosh','JameerNelson']
ASG09 = ['LeBronJames','DwyaneWade','DwightHoward','JoeJohnson','KevinGarnett','ChrisBosh','RajonRondo','GeraldWallace','DerrickRose',
         'AlHorford','DavidLee','PaulPierce','CarmeloAnthony','DirkNowitzki','SteveNash',"AmareStoudemire",'TimDuncan',
         'DeronWilliams','ChaunceyBillups','PauGasol','KevinDurant','ZachRandolph','ChrisKaman','JasonKidd',
         'AllenIverson','KobeBryant','ChrisPaul','BrandonRoy']
ASG10 = ['KevinDurant','KobeBryant','ChrisPaul','CarmeloAnthony','TimDuncan','PauGasol','ManuGinóbili','DeronWilliams',
         'BlakeGriffin','DirkNowitzki','RussellWestbrook','KevinLove','LeBronJames','DerrickRose',"AmareStoudemire",
         'DwightHoward','DwyaneWade','ChrisBosh','RajonRondo','JoeJohnson','RayAllen','PaulPierce','AlHorford','KevinGarnett',
         'YaoMing']
ASG11 = ['KevinDurant','KobeBryant','BlakeGriffin','ChrisPaul','AndrewBynum','RussellWestbrook','KevinLove','DirkNowitzki','MarcGasol',
         'TonyParker','LaMarcusAldridge','SteveNash','DwyaneWade','LeBronJames','DwightHoward','CarmeloAnthony','DerrickRose',
         'ChrisBosh','DeronWilliams','RajonRondo','AndreIguodala','PaulPierce','RoyHibbert','LuolDeng','JoeJohnson']
ASG12 = ['KevinDurant','KobeBryant','BlakeGriffin','ChrisPaul','DwightHoward','JamesHarden','TonyParker','RussellWestbrook',
         'DavidLee','ZachRandolph','LaMarcusAldridge','TimDuncan','CarmeloAnthony','LeBronJames','DwyaneWade','ChrisBosh',
         'KevinGarnett','KyrieIrving','PaulGeorge','LuolDeng','TysonChandler','JoakimNoah','JrueHoliday','BrookLopez','RajonRondo']
ASG13 = ['KyrieIrving','LeBronJames','PaulGeorge','CarmeloAnthony','DwyaneWade','JoakimNoah', 'JohnWall','DeMarDeRozan',
         'PaulMillsap','RoyHibbert','ChrisBosh','JoeJohnson','KevinDurant','KevinLove','BlakeGriffin','StephenCurry',
         'JamesHarden','ChrisPaul','DwightHoward','LaMarcusAldridge','TonyParker','AnthonyDavis','DamianLillard',
         'DirkNowitzki','KobeBryant']
ASG14 = ['JamesHarden','StephenCurry','MarcGasol','KlayThompson','LaMarcusAldridge','ChrisPaul','RussellWestbrook',
         'DeMarcusCousins','DamianLillard','TimDuncan','DirkNowitzki','KevinDurant','LeBronJames','CarmeloAnthony','JohnWall',
         'PauGasol','KyleLowry','KyrieIrving','KyleKorver','PaulMillsap','AlHorford','JeffTeague','ChrisBosh','JimmyButler',
         'DwyaneWade','KobeBryant','AnthonyDavis','BlakeGriffin']
ASG15 = ['StephenCurry','KobeBryant','KawhiLeonard','KevinDurant','RussellWestbrook','JamesHarden','KlayThompson',
         'ChrisPaul','AnthonyDavis','LaMarcusAldridge','DraymondGreen','DeMarcusCousins','KyleLowry','PaulGeorge',
         'CarmeloAnthony','LeBronJames','JohnWall','PaulMillsap','IsaiahThomas','DeMarDeRozan','AndreDrummond','PauGasol',
         'AlHorford','ChrisBosh','JimmyButler']
ASG16 = ['AnthonyDavis','JamesHarden','StephenCurry','KevinDurant','KawhiLeonard','MarcGasol','RussellWestbrook',
         'KlayThompson','GordonHayward','DraymondGreen','DeAndreJordan','DeMarcusCousins','DeMarDeRozan',
         'GiannisAntetokounmpo','KyrieIrving','LeBronJames','JimmyButler','PaulGeorge','JohnWall','KembaWalker',
         'CarmeloAnthony','IsaiahThomas','KyleLowry','PaulMillsap','KevinLove']
ASG17 = ['LeBronJames','KevinDurant','RussellWestbrook','KyrieIrving','AnthonyDavis','PaulGeorge','BradleyBeal',
         'AndreDrummond','VictorOladipo','KembaWalker','GoranDragić','LaMarcusAldridge','JamesHarden','DeMarDeRozan',
         'StephenCurry','GiannisAntetokounmpo','JoelEmbiid','KyleLowry','KlayThompson','DamianLillard','DraymondGreen',
         'KarlAnthonyTowns','AlHorford','DeMarcusCousins','KevinLove','KristapsPorziņģis','JohnWall','JimmyButler']
ASG18 = ['LeBronJames','JamesHarden','KevinDurant','KyrieIrving','KawhiLeonard','DamianLillard','KlayThompson',
         'BradleyBeal','BenSimmons','KarlAnthonyTowns','LaMarcusAldridge','DwyaneWade','AnthonyDavis','StephenCurry',
         'GiannisAntetokounmpo','PaulGeorge','JoelEmbiid','KembaWalker','KhrisMiddleton','BlakeGriffin','RussellWestbrook',
         'NikolaJokić','KyleLowry',"DAngeloRussell",'NikolaVučević','DirkNowitzki','VictorOladipo']
ASG19 = ['KawhiLeonard','AnthonyDavis','LeBronJames','JamesHarden','LukaDončić','BenSimmons','RussellWestbrook',
         'ChrisPaul','DevinBooker','DomantasSabonis','JaysonTatum','NikolaJokić','GiannisAntetokounmpo','KembaWalker',
         'JoelEmbiid','PascalSiakam','TraeYoung','KyleLowry','KhrisMiddleton','RudyGobert','DonovanMitchell','JimmyButler',
         'BamAdebayo','BrandonIngram','DamianLillard']
ASG20 = ['LukaDončić','StephenCurry','GiannisAntetokounmpo','NikolaJokić','LeBronJames','ChrisPaul','JaylenBrown','PaulGeorge',
         'DamianLillard','DomantasSabonis','RudyGobert','KyrieIrving','BradleyBeal','KawhiLeonard','JaysonTatum',
         'ZionWilliamson','JamesHarden','DonovanMitchell','ZachLaVine','NikolaVučević','JuliusRandle','MikeConley',
         'DevinBooker','AnthonyDavis','KevinDurant','JoelEmbiid','BenSimmons']
ASG21 = ['StephenCurry','LeBronJames','GiannisAntetokounmpo','DeMarDeRozan','NikolaJokić','LukaDončić','DariusGarland',
         'JarrettAllen','FredVanVleet','JimmyButler','ChrisPaul','JoelEmbiid','JaysonTatum','TraeYoung','JaMorant','AndrewWiggins',
         'DevinBooker','DejounteMurray','LaMeloBall','KhrisMiddleton','KarlAnthonyTowns','RudyGobert','ZachLaVine','KevinDurant',
         'DraymondGreen','JamesHarden','DonovanMitchell']
```

```{python, echo=F}
s03['AllStar'] = np.where(np.isin(s03['Player'],ASG03), 1, 0)
s04['AllStar'] = np.where(np.isin(s04['Player'],ASG04), 1, 0)
s05['AllStar'] = np.where(np.isin(s05['Player'],ASG05), 1, 0)
s06['AllStar'] = np.where(np.isin(s06['Player'],ASG06), 1, 0)
s07['AllStar'] = np.where(np.isin(s07['Player'],ASG07), 1, 0)
s08['AllStar'] = np.where(np.isin(s08['Player'],ASG08), 1, 0)
s09['AllStar'] = np.where(np.isin(s09['Player'],ASG09), 1, 0)
s10['AllStar'] = np.where(np.isin(s10['Player'],ASG10), 1, 0)
s11['AllStar'] = np.where(np.isin(s11['Player'],ASG11), 1, 0)
s12['AllStar'] = np.where(np.isin(s12['Player'],ASG12), 1, 0)
s13['AllStar'] = np.where(np.isin(s13['Player'],ASG13), 1, 0)
s14['AllStar'] = np.where(np.isin(s14['Player'],ASG14), 1, 0)
s15['AllStar'] = np.where(np.isin(s15['Player'],ASG15), 1, 0)
s16['AllStar'] = np.where(np.isin(s16['Player'],ASG16), 1, 0)
s17['AllStar'] = np.where(np.isin(s17['Player'],ASG17), 1, 0)
s18['AllStar'] = np.where(np.isin(s18['Player'],ASG18), 1, 0)
s19['AllStar'] = np.where(np.isin(s19['Player'],ASG19), 1, 0)
s20['AllStar'] = np.where(np.isin(s20['Player'],ASG20), 1, 0)
s21['AllStar'] = np.where(np.isin(s21['Player'],ASG21), 1, 0)
```

```
s03['AllStar'] = np.where(np.isin(s03['Player'],ASG03), 1, 0)
```
```{python, echo=F, results='hide'}
#Make sure all the all stars were labeled properly
s06.AllStar.value_counts()
```

Now that each of the individual datasets have been properly manipulated with the target variable added, we can concatenate them together to form our training dataset.

```{python}
#re-establish dfList with updated DFs
dfList = [s03,s04,s05,s06,s07,s08,s09,s10,s11,s12,s13,s14,s15,s16,s17,s18,s19,s20,s21]
#Concatenate the training seasons
train = pd.concat(dfList)
```

Quickly we will check the distribution of All-Stars(1) in our overall training set.
```{python}
train['AllStar'].value_counts()
#roughly 8% of the data is AllStar, may not need to oversample
```
Next, I applied a label encoder to ensure the target variable is an  appropriate binary object for modeling
purposes. 
```{python}
#Change numeric to binary target
label_encoder = LabelEncoder()
train['AllStar'] = label_encoder.fit_transform(train['AllStar'])
```

Since we have added the appropriate All-Star labels to all players in our training set, we can now
drop the player name variable, as it does not provide any predictive power.

```{python}
#Drop player col
train = train.drop(['Player'], axis=1)
```

Next, I noticed the dataset took many values for the position variable, as some players are listed as
playing two positions (ex: "PG-SG"). To clean this up, I combined levels into the guard and frontcourt
labels we will use for our predicted roster selection.

```{python}
#Check which values of position are taken across training set for aggregation
train.Pos.value_counts()
```

```{python}
#Combine Guard/Front-Court positions
train['Pos'] = np.where(((train['Pos'].eq('PG')) | (train['Pos'].eq('SG')) | (train['Pos'].eq('PG-SG')) |(train['Pos'].eq('SG-PG')) |
                         (train['Pos'].eq('SG-PF'))|
    (train['Pos'].eq('SG-PG-SF'))),'Guard','Frontcourt')
```

Tree based models like an XGBoost generally do not do well handling categorical variables. For this
reason, I used One-Hot Encoding on our categorical position variable. 
```{python}
transformer = make_column_transformer(
    (OneHotEncoder(sparse=False), ['Pos']),
    remainder = 'passthrough'
)
transformed = transformer.fit_transform(train)
train = pd.DataFrame(transformed, columns=transformer.get_feature_names_out())
```

**Test Dataset**

Now that our training dataset is finalized, we can create our test dataset with the current season's
data. As always in a modeling project, we must manipulate the test set in the same manner we did our training
set. 

```{python}
#Test set read in
test = pd.read_csv('C:/Users/ericd/OneDrive - North Carolina State University/Desktop/NBA-AllStar-Predictions/Data/season22-23.csv')
```
Similar to with the training set, I created a new function to apply all manipulations to the test set at once, including the scaling, position variable cleaning, and One-Hot Encoding.

```{python}
#Test set cleaning function
def testPrep(df):
    df = df.sort_values(['Player','G'], ascending = [True, False])
    df = df.drop_duplicates(subset=['Player'], keep='first')
    df = df[df['MP'] >= 15]
    df = df.drop(['Rk','Tm'], axis=1)
    df['Age'] = zscore(df['Age'])
    df['G'] = zscore(df['G'])
    df['GS'] = zscore(df['GS'])
    df['MP'] = zscore(df['MP'])
    df['FG'] = zscore(df['FG'])
    df['FGA'] = zscore(df['FGA'])
    df['FG%'] = zscore(df['FG%'])
    df['3P'] = zscore(df['3P'])
    df['3PA'] = zscore(df['3PA'])
    df['2P'] = zscore(df['2P'])
    df['2PA'] = zscore(df['2PA'])
    df['2P%'] = zscore(df['2P%'])
    df['eFG%'] = zscore(df['eFG%'])
    df['FT'] = zscore(df['FT'])
    df['FTA'] = zscore(df['FTA'])
    df['FT%'] = zscore(df['FT%'])
    df['ORB'] = zscore(df['ORB'])
    df['DRB'] = zscore(df['DRB'])
    df['TRB'] = zscore(df['TRB'])
    df['AST'] = zscore(df['AST'])
    df['STL'] = zscore(df['STL'])
    df['BLK'] = zscore(df['BLK'])
    df['TOV'] = zscore(df['TOV'])
    df['PF'] = zscore(df['PF'])
    df['PTS'] = zscore(df['PTS'])

    df['Pos'] = np.where(((df['Pos'].eq('PG')) | (df['Pos'].eq('SG'))),'Guard','Frontcourt')

    #one hot encoding for test
    transformer = make_column_transformer(
    (OneHotEncoder(sparse=False), ['Pos']),
    remainder = 'passthrough'
    )
    transformed = transformer.fit_transform(df)
    df = pd.DataFrame(transformed, columns=transformer.get_feature_names_out())
    return df
```

I then ran the test set through the function defined above.
```{python}
#Send test set through cleaning function
test = testPrep(test)
```

```{python, echo=F}
playernames = test['remainder__Player'].tolist()
test = test.drop(['remainder__Player'],axis=1)
```

```{python, echo=F}
s = test.select_dtypes(include='object').columns
test[s] = test[s].astype("float")
```

## Exploratory Data Analysis (EDA)

Now it was time to look at a few of our variables at a closer level. Intuitively we can expect that All-Stars 
generally have better stats than the average player; it is also important to remember that minutes played per game
likely plays a large role in statistics as well. Let's take a look at some of these variables visually.


First, let's see how minutes and points correlate. 
```{python, echo=F}
plt.scatter(x=train['remainder__MP'], y=train['remainder__PTS'])
plt.title('Scatterplot of Minutes/Game vs. Points/Game')
plt.xlabel('Minutes/Game (Scaled)')
plt.ylabel('Points/Game (Scaled)')
```

As we can see, there is a pretty clear positive, linear relationship between the variables. However, one thing 
to note is how after the scaled minutes per game value exceeds 1, the points per game values seem to increase at a
slightly steeper rate. This could be, as previously mentioned, the fact that the best players play the most 
minutes and are also the ones scoring the most points. 



Next we will take a look at how points per game looks across our All-Stars versus non All-Stars. 
```{python}
sns.boxplot(x = train['remainder__AllStar'],
            y = train['remainder__PTS'])
plt.title('Boxplot of Points/Game by All-Star Status')
plt.xlabel('All-Star Status')
plt.ylabel('Points/Game (Scaled)')
```
Unsurprisingly, All-Stars generally score at a much more standout level than their peers. In fact, 
the median scoring value for an All-Star is about 2 standard deviations from league standard, which encompasses
roughly the top 5% of scorers in the league. It is also noted that there were outliers for All-Stars that scored
below the league standard. 



Next, let's look at how field goal % compares across All-Star status.
```{python, echo=F}
sns.boxplot(x = train['remainder__AllStar'],
            y = train['remainder__FG%'])
plt.title('Boxplot of Field Goal % by All-Star Status')
plt.xlabel('All-Star Status')
plt.ylabel('FG % (Scaled)')
```

Interesting to see how the medians across both groups are roughly at 0, or league standard. However, the most 
notable observation here is how the spread of the non-All-Stars is vastly larger than the All-Stars, stretching
down as far as nearly 8 standard deviations below the mean.



Lastly, let's look at a correlation matrix across all of our variables.
```{python, echo=F}
corr_matrix = train.corr()
sns.heatmap(corr_matrix, annot=False)
plt.title('Correlation Matrix')
```

While a bit difficult to interpret easily, there are a few interesting takeaways. First, Field Goal % seems to
have a strong, negative correlation with other shooting statistics. This may represent players who 
take very few shots, but make them at a high clip, leaving them with high shooting percentages with low attempts.
We also see that the age variable seems to have no notable correlations with any other variable. I would have 
expected to see stats, especially percentages, drop as a player ages, but evidently, this is not as glaring of
a trend as I thought.


## Modeling

Now that our training/test datasets are prepared, and we have explored our data, it is time to set up our modeling
procedure. 

**Data Split**


First, we will create our X and Y objects. The X object will contain only our predictor variables, while the Y
object will be a list containing the values of the target variable.
```{python}
#Create X and Y object for data split function
X = train.drop(['remainder__AllStar'],axis=1)
Y = train['remainder__AllStar'].values
```

Since we do not have the All-Star values for our test set at the time of this writing, I chose to split our training
set into a training and validation set. The true training set will be used to build the model, with the 
validation set being used for model assessment. The code to split these datasets is shown below.

```{python}
#data split
X_train, X_valid, Y_train, Y_valid = train_test_split(X, Y, test_size = .2, stratify=Y, random_state=20)
```

**Model Fitting**


Next, I will establish our XGBoost regressor object.
```{python}
#Establish XGB regressor
xgb_model = xgb.XGBRegressor(objective="binary:logistic", random_state=0)
```

Cross-validation is a key technique to apply in model building to help avoid over-fitting on the training set. For
our model, I will use 10-fold cross-validation, repeated 3 times. 
```{python}
#Set CV params
cv = RepeatedStratifiedKFold(n_splits = 10, n_repeats = 3, random_state=0)
```

I decided to utilize a small grid search across tuning hyperparameters for the model. This allows us to try out
many combinations of key aspects of an XGBoost, such as the depth of the trees and the number of trees in the forest. 
```{python}
#Establish tuning parameters
params = {
    "max_depth":[10,15,20],
    "n_estimators":[200,400,600],
    "min_samples_split":[5,10]
}
```

Next, I set our grid search object with the tuning parameters and cross-validation specifics. I chose the area under
the ROC curve (AUROC) to serve as our cross-validation metric.
```{python}
#Set gridsearch object. Use ROC as our CV measure
clf = GridSearchCV(xgb_model, params, cv=cv, scoring='roc_auc')
```

Now it is time to run the model.
```{python, results='hide'}
#Fit model
tick = time.time()
clf.fit(X_train,Y_train)
tock = time.time()

print("Fit complete")
print(f"Time Taken: {round((tock-tick)/60,2)} minutes")
```

With the grid search complete, we will take the best outcome and store it as our final model, as well as 
look at the selected hyperparameters. 
```{python}
#Take best model from grid search
final_model = clf.best_estimator_
```

```{python}
#View parameters of the selected model
final_model.get_params()
```

**Model Assessment**

With our final model set, let's see how it did on our validation dataset by plotting the area under the ROC curve.
```{python, echo=F}
#Store predictions on validation set
Y_pred = final_model.predict(X_valid)
```

```{python, echo=F, results='hide'}
#Find metrics via roc curve function
fpr, tpr, thresh = roc_curve(Y_valid, Y_pred)
```

```{python, echo=F, results='hide'}
#Numerically view area under ROC score
auc(fpr,tpr)
```

```{python, echo=F}
#Plot ROC Curve
plt.figure(figsize=(8,4))
plt.plot(fpr,tpr, lw=2, color='red')
plt.xlim([0,1])
plt.ylim([0,1])
plt.xlabel("FPR")
plt.ylabel("TPR")
plt.title("ROC Curve for XGBoost", size=14)
plt.text(s=f'{round(auc(fpr,tpr),2)}', x=.5, y=.5)
plt.fill_between(fpr,tpr, alpha=0.2)
```

With an AUROC value of *.98*, we can feel confident that our model did a solid job  distinguishing
our All-Stars from the non All-Stars. 

**Variable Importance**

Let's now look at which variables had the biggest impact on our model's predictions. We will limit this
to the top 10 most impactful predictors.

```{python, echo=F}
#Pull variable importance from the model output, rank by most important
importance = pd.DataFrame(final_model.feature_importances_, index=list(X_train.columns))
importance.columns = ['importance']
importance.sort_values(by='importance', ascending=False, inplace=True)
```


```{python, echo=F}
#plot importance
sns.barplot(x='importance', y=importance.index, data=importance, order=importance.index[:10])
plt.title('10 Most Important Variables')
plt.xlabel('Importance Score')
```



Unsurprisingly, points per game was by far the most important variable to our model's predictions. Intuitively,
seeing field goals made per game following it in second place makes sense as well, as the variables would be 
highly correlated (more made shots = more points). 

## Results
```{python, echo=F}
#Store test predictions
Y_pred_test = final_model.predict(test)
resultsList = {'Player':playernames, 'All-Star Probability':Y_pred_test}
results = pd.DataFrame(resultsList)

#merge probabilities on to original '22-'23 dataset with 
final = pd.read_csv('C:/Users/ericd/OneDrive - North Carolina State University/Desktop/NBA-AllStar-Predictions/Data/season22-23.csv')

final = final.merge(results, on='Player',how='left')

#Position grouping
final['Pos'] = np.where((final['Pos'].eq('PG')) | (final['Pos'].eq('SG')),'Guard','Frontcourt')

#Conference Grouping
west = ['MEM','NOP','DEN','PHO','POR','SAC','UTA','LAC','DAL','GSW','MIN','LAL','OKC','SAS','HOU']
final['Conference'] = np.where(np.isin(final['Tm'],west), 'West', 'East')

#Sort by descending ASG probability
final = final.sort_values(by='All-Star Probability', ascending=False)
```

**Predictions**

Now we are ready to see what predicted probabilities our model assigns to our current players. We will split
the test set by East and West Conference players, then rank them by All-Star probability.

First, our top Eastern All-Star Candidates:
```{python, echo=F}
#East All-Stars
final[final['Conference'] == 'East'].head(15)[['Player','Pos','All-Star Probability']]
```
Next, for the Western Conference.
```{python, echo=F}
#East All-Stars
final[final['Conference'] == 'West'].head(15)[['Player','Pos','All-Star Probability']]
```

Now, there are a few initial reactions I have to these results. First, the model has done a great
job at being near certain that the league's most elite players will be All-Stars. Stephen Curry,
Kevin Durant, and Jayson Tatum are all players who find themselves consistently in the conversation
for Most Valuable Player. They are near-locks to be an All-Star every season. Our model identified
top performers such as them, giving them over a 99% probability of making the All-Star team.

What I find most interesting is how quickly the predicted probabilities drop off after you get past
the elite. For example, in our Eastern Conference table, after Zion Williamson (69.2%% probability),
the next highest drops to Lauri Markkanen at just over 8%. In fact, even the 15th highest
probability in the conference, De'Aaron Fox, has only a 2.4% chance of making it. This tells that
our model will generally assign incredibly high or low probabilities to players. I believe that
through these results, we can segment the league into three rough categories:

- Elite All-Star (Top 15 player)

- Fringe All-Star (16-35th best player) 

- Rest of league (36th and lower)

Top players will be clear standouts to make the cut with little to no debate. These should be
obvious even to the casual fan. However, there are likely 20 or so players with above-average stats
but do not stand out as elite. Players of this type are much more common than the elite, however
only have roughly 5 All-Star spots per conference to fill. Because of this, the individual
probability of this type of player making the All-Star game is likely much lower, explaining the
significant drop-off in probabilities in both conferences. Based on the model output, I would
suspect that any player with above a 1% chance of making the All-Star game is likely a top 50 player
across the league, despite how low this value sounds.

**Limitations**

While generally pleased with the model results, I would like to address a few areas of limitation.
First, the selection criteria. While this model makes predictions from an objective, statistical standpoint, this is not how it is selected in practice. As mentioned in the background section, All-Stars are selected via a mix of the fan, player, media, and coach votes. Subjectivity, especially for super-fans who spend hours submitting numerous votes for their favorite players, can cause some skew in the results. This may also apply to the players and coaches who hold some implicit bias towards their teammates or friends in the league. Since voting procedures have changed over the years, it would be incredibly challenging to feature engineer this into the dataset for improved predictions.

Another concern is the rare occasion in which an NBA legend retires. Generally, as shown over the years, well-loved and celebrated players are selected for the All-Star game, despite their stats not showing them as a standout. An example of this that was mentioned earlier is Dirk Nowitzki being selected as an All-Star in 2018-19. While only playing 15 minutes per game and averaging less than 8 points per game, Dirk still made the roster. Similar to Kobe Bryant ( 17 points per game on 35% shooting) and Dwyane Wade (15 points per game) in 2016 and 2018, respectively, this act of respect to an outgoing legend does serve as an outlier. While infrequent, the model takes all instances of an All-Star selection into account without this consideration without this added context.

**Next Steps**

I thoroughly enjoyed working on this project and am greatly pleased with the results of the models' predictive power. With that being said, the fun part has yet to come. At the time of this writing (Dec. 2022), I plan to update the model’s predictions two times between now and February, just before the All-Star rosters are officially announced. I hope to store the predictions from each time point to track and compare which players are the biggest risers and fallers as the season continues and more data rolls in. Come February, I hope to present a final All-Star roster prediction, in which I will assess the accuracy of following the announcements.

