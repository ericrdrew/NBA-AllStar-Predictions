[["index.html", "NBA All-Star Predictions 1 NBA All-Star Predictions 1.1 Introduction 1.2 Methodology/Data Prep", " NBA All-Star Predictions Eric Drew 2022-12-19 1 NBA All-Star Predictions 1.1 Introduction Objective Utilize an Extreme Gradient Boosting model, hereafter referred to as “XGBoost”, to predict the NBA All-Star roster for the 2022-23 season. Background Each season in February, 24 of the NBA’s top performing players are selected as All-Stars. 12 players represent each of the East and West Conferences with the following breakdown: Four guards (Point Guard, Shooting Guard) Six frontcourt players (Small Forward, Power Forward, Center) Two additional players regardless of position Starting lineups are selected by a combination of fan, current player, and media votes, while the reserve players are selected by the league’s 30 head coaches. Injured players will be replaced by a player selected by the league’s Commissioner, Adam Silver [1]. Data The model will be built off of player’s “per game” statistics from the 2003-04 to 2021-22 seasons, accessed from the public sports database Basketball-Reference [2]. Player statistics from the current season, which is only ~30 games in at the time of writing, will be used as our test dataset. My goal is to update my predictions in January and again in February just before the rosters are announced, allowing me to assess my models predictive power in real-time. While the model will not change during this time period, new data will be available as players continue to complete more games in the coming months. 1.2 Methodology/Data Prep I will be utilizing a combination of common and advanced Python libraries to complete the project. The following code is living and may be adapted as the project continues. import pandas as pd import numpy as np import matplotlib.pyplot as plt import time import sklearn import seaborn as sns import xgboost as xgb from scipy.stats import zscore from sklearn.preprocessing import OneHotEncoder, LabelEncoder from sklearn.compose import make_column_transformer from sklearn.model_selection import GridSearchCV, KFold, RepeatedStratifiedKFold, train_test_split, cross_val_score, RandomizedSearchCV from sklearn.metrics import roc_curve, auc, roc_auc_score, RocCurveDisplay, plot_confusion_matrix from sklearn.calibration import calibration_curve Now we will read in our 19 seasons of player per game statistics to build our training set on. s03 = pd.read_csv(&#39;C:/Users/ericd/OneDrive - North Carolina State University/Desktop/NBA-AllStar-Predictions/Data/season03-04.csv&#39;) s04 = pd.read_csv(&#39;C:/Users/ericd/OneDrive - North Carolina State University/Desktop/NBA-AllStar-Predictions/Data/season04-05.csv&#39;) s05 = pd.read_csv(&#39;C:/Users/ericd/OneDrive - North Carolina State University/Desktop/NBA-AllStar-Predictions/Data/season05-06.csv&#39;) s06 = pd.read_csv(&#39;C:/Users/ericd/OneDrive - North Carolina State University/Desktop/NBA-AllStar-Predictions/Data/season06-07.csv&#39;) s07 = pd.read_csv(&#39;C:/Users/ericd/OneDrive - North Carolina State University/Desktop/NBA-AllStar-Predictions/Data/season07-08.csv&#39;) s08 = pd.read_csv(&#39;C:/Users/ericd/OneDrive - North Carolina State University/Desktop/NBA-AllStar-Predictions/Data/season08-09.csv&#39;) s09 = pd.read_csv(&#39;C:/Users/ericd/OneDrive - North Carolina State University/Desktop/NBA-AllStar-Predictions/Data/season09-10.csv&#39;) s10 = pd.read_csv(&#39;C:/Users/ericd/OneDrive - North Carolina State University/Desktop/NBA-AllStar-Predictions/Data/season10-11.csv&#39;) s11 = pd.read_csv(&#39;C:/Users/ericd/OneDrive - North Carolina State University/Desktop/NBA-AllStar-Predictions/Data/season11-12.csv&#39;) s12 = pd.read_csv(&#39;C:/Users/ericd/OneDrive - North Carolina State University/Desktop/NBA-AllStar-Predictions/Data/season12-13.csv&#39;) s13 = pd.read_csv(&#39;C:/Users/ericd/OneDrive - North Carolina State University/Desktop/NBA-AllStar-Predictions/Data/season13-14.csv&#39;) s14 = pd.read_csv(&#39;C:/Users/ericd/OneDrive - North Carolina State University/Desktop/NBA-AllStar-Predictions/Data/season14-15.csv&#39;) s15 = pd.read_csv(&#39;C:/Users/ericd/OneDrive - North Carolina State University/Desktop/NBA-AllStar-Predictions/Data/season15-16.csv&#39;) s16 = pd.read_csv(&#39;C:/Users/ericd/OneDrive - North Carolina State University/Desktop/NBA-AllStar-Predictions/Data/season16-17.csv&#39;) s17 = pd.read_csv(&#39;C:/Users/ericd/OneDrive - North Carolina State University/Desktop/NBA-AllStar-Predictions/Data/season17-18.csv&#39;) s18 = pd.read_csv(&#39;C:/Users/ericd/OneDrive - North Carolina State University/Desktop/NBA-AllStar-Predictions/Data/season18-19.csv&#39;) s19 = pd.read_csv(&#39;C:/Users/ericd/OneDrive - North Carolina State University/Desktop/NBA-AllStar-Predictions/Data/season19-20.csv&#39;) s20 = pd.read_csv(&#39;C:/Users/ericd/OneDrive - North Carolina State University/Desktop/NBA-AllStar-Predictions/Data/season20-21.csv&#39;) s21 = pd.read_csv(&#39;C:/Users/ericd/OneDrive - North Carolina State University/Desktop/NBA-AllStar-Predictions/Data/season21-22.csv&#39;) There were a handful of things that needed to be done to the datasets before I could concatenate all 19 seasons into the cumulative training set. Scaling by Season The NBA is an incredibly dynamic league and has changed greatly over the last decades. For example, the average points scored by a team per game has fluctuated a good bit just in the time horizon we are concerned with in this project, with a high of 112.1 PPG in 2020-21 and low of 93.4 PPG in 2003-04, a difference of nearly 20 points per game [3]. Because of changes like these, it is difficult to compare raw statistics between players across seasons, especially those over a decade apart. For example, a player scoring 20 PPG in 2003 was likely more impactful than a 20 PPG scorer in 2020, and in the context of our project’s scope, may be the difference between an All-Star or not. The question of whether a player is an All-Star is likely not as simple as “how many points per game do they score?”, but “how does their scoring average compare to the rest of the league?” To combat this challenge, I scaled each season’s data individually using Z-scores, allowing us to measure how strong a player’s stat line was compared to the league that season. We then will be able to compare players across time by how much they stood out across a distribution, regardless of the season they played in. Examples of these calculations are shown below. df[&#39;PTS&#39;] = zscore(df[&#39;PTS&#39;]) df[&#39;Age&#39;] = zscore(df[&#39;Age&#39;])` df[&#39;PF&#39;] = zscore(df[&#39;PF&#39;]) Observation Exclusion In exploring the data, one challenge I discovered in the data was that for players who were traded or played for multiple seasons had more than one observation. These players had observations representing their stats while playing for each team during a season, as well as a cumulative season stat line, which was represented by their team being “TOT” for total. For the purposes of my model, I dropped the individual team stat line observations, and kept only the total season stats for a player. df = df.sort_values([&#39;Player&#39;,&#39;G&#39;], ascending = [True, False]) df = df.drop_duplicates(subset=[&#39;Player&#39;], keep=&#39;first&#39;) Another concern was the data being too imbalanced for a model to find meaningful signal in. Most NBA seasons see over 450 individual players log minutes in at least one game. Left as this, the league would contain roughly ~5% All-Stars, crossing into rare-event territory. Seeing as players who rarely see the court are not going to be in contention for making the All-Star roster as is, I decided to subset our data to retain only players who played at least 15 minutes per 48 minute game. This was in an attempt to reduce the noise caused by an overwhelming amount of players who only enter games for a handful of minutes per game at best, who’s statistics would intuitively not bring any predictive power in separating All-Stars versus those who are not. df = df[df[&#39;MP&#39;] &gt;= 15] Lastly, there were some miscellaneous manipulations that needed to be made to the datasets, such dropping irrelevant variables such as an ID variable, and removing random special characters found in some observations. df = df.drop([&#39;Rk&#39;,&#39;Player-additional&#39;,&#39;Tm&#39;], axis=1) df[&#39;Player&#39;] = df[&#39;Player&#39;].str.replace(&#39;\\W&#39;, &#39;&#39;, regex=True) To more efficiently apply these manipulations across all 19 datasets, I created a function to run them each through, as opposed to writing these commands for each one. The code for this, which is a culmination of the above commands, is shown below. #create list of dataframes to be used in loops(TRAIN ONLY) dfList = [s03,s04,s05,s06,s07,s08,s09,s10,s11,s12,s13,s14,s15,s16,s17,s18,s19,s20,s21] #cleaning function removing TOTs, keep to players &gt;=15MPG, scale continuous variables, remove special characters on name def trainPrep(df): df = df.drop([&#39;Rk&#39;,&#39;Player-additional&#39;,&#39;Tm&#39;], axis=1) df = df.sort_values([&#39;Player&#39;,&#39;G&#39;], ascending = [True, False]) df = df.drop_duplicates(subset=[&#39;Player&#39;], keep=&#39;first&#39;) df = df[df[&#39;MP&#39;] &gt;= 15] df[&#39;Age&#39;] = zscore(df[&#39;Age&#39;]) df[&#39;G&#39;] = zscore(df[&#39;G&#39;]) df[&#39;GS&#39;] = zscore(df[&#39;GS&#39;]) df[&#39;MP&#39;] = zscore(df[&#39;MP&#39;]) df[&#39;FG&#39;] = zscore(df[&#39;FG&#39;]) df[&#39;FGA&#39;] = zscore(df[&#39;FGA&#39;]) df[&#39;FG%&#39;] = zscore(df[&#39;FG%&#39;]) df[&#39;3P&#39;] = zscore(df[&#39;3P&#39;]) df[&#39;3PA&#39;] = zscore(df[&#39;3PA&#39;]) df[&#39;2P&#39;] = zscore(df[&#39;2P&#39;]) df[&#39;2PA&#39;] = zscore(df[&#39;2PA&#39;]) df[&#39;2P%&#39;] = zscore(df[&#39;2P%&#39;]) df[&#39;eFG%&#39;] = zscore(df[&#39;eFG%&#39;]) df[&#39;FT&#39;] = zscore(df[&#39;FT&#39;]) df[&#39;FTA&#39;] = zscore(df[&#39;FTA&#39;]) df[&#39;FT%&#39;] = zscore(df[&#39;FT%&#39;]) df[&#39;ORB&#39;] = zscore(df[&#39;ORB&#39;]) df[&#39;DRB&#39;] = zscore(df[&#39;DRB&#39;]) df[&#39;TRB&#39;] = zscore(df[&#39;TRB&#39;]) df[&#39;AST&#39;] = zscore(df[&#39;AST&#39;]) df[&#39;STL&#39;] = zscore(df[&#39;STL&#39;]) df[&#39;BLK&#39;] = zscore(df[&#39;BLK&#39;]) df[&#39;TOV&#39;] = zscore(df[&#39;TOV&#39;]) df[&#39;PF&#39;] = zscore(df[&#39;PF&#39;]) df[&#39;PTS&#39;] = zscore(df[&#39;PTS&#39;]) df[&#39;Player&#39;] = df[&#39;Player&#39;].str.replace(&#39;\\W&#39;, &#39;&#39;, regex=True) return df #Run training sets through cleaning function s03,s04,s05,s06,s07,s08,s09,s10,s11,s12,s13,s14,s15,s16,s17,s18,s19,s20,s21 =[trainPrep(df) for df in dfList] Target Variable Creation Next, I created lists of each of the All-Star rosters from the past 19 seasons. I then assigned a new variable, ‘AllStar’, to each player in each dataset. Players who were on the All-Star roster for that season received a value of 1, while players who were not All-Stars received a 0. An example of this is shown below. The object “ASG03” represents the list of All-Stars from the 2003-04 season. s03[&#39;AllStar&#39;] = np.where(np.isin(s03[&#39;Player&#39;],ASG03), 1, 0) ## 0 263 ## 1 25 ## Name: AllStar, dtype: int64 Now that each of the individual datasets have been properly manipulated with the target variable added, we can concatenate them together to form our training dataset. #re-establish dfList with updated DFs dfList = [s03,s04,s05,s06,s07,s08,s09,s10,s11,s12,s13,s14,s15,s16,s17,s18,s19,s20,s21] #Concatenate the training seasons train = pd.concat(dfList) Quickly we will check the distribution of All-Stars(1) in our overall training set. train[&#39;AllStar&#39;].value_counts() #roughly 8% of the data is AllStar, may not need to oversample ## 0 5563 ## 1 494 ## Name: AllStar, dtype: int64 Next, I applied a label encoder to ensure the target variable is an appropriate binary object for modeling purposes. #Change numeric to binary target label_encoder = LabelEncoder() train[&#39;AllStar&#39;] = label_encoder.fit_transform(train[&#39;AllStar&#39;]) Since we have added the appropriate All-Star labels to all players in our training set now, we can now drop the player name variable, as it does not provide any predictive power. #Drop player col train = train.drop([&#39;Player&#39;], axis=1) Next, I noticed the dataset took many values for the position variable, as some players are listed as playing two positions (ex: “PG-SG”). To clean this up, I combined levels into the guard and frontcourt labels we will use for our predicted roster selection. #Check which values of position are taken across training set for aggregation train.Pos.value_counts() ## SG 1335 ## PG 1207 ## PF 1158 ## SF 1134 ## C 1085 ## SF-SG 23 ## PG-SG 21 ## SG-PG 18 ## PF-SF 16 ## C-PF 14 ## SG-SF 14 ## PF-C 13 ## SF-PF 13 ## SG-PF 4 ## PG-SF 1 ## SG-PG-SF 1 ## Name: Pos, dtype: int64 #Combine Guard/Front-Court positions train[&#39;Pos&#39;] = np.where(((train[&#39;Pos&#39;].eq(&#39;PG&#39;)) | (train[&#39;Pos&#39;].eq(&#39;SG&#39;)) | (train[&#39;Pos&#39;].eq(&#39;PG-SG&#39;)) |(train[&#39;Pos&#39;].eq(&#39;SG-PG&#39;)) | (train[&#39;Pos&#39;].eq(&#39;SG-PF&#39;))| (train[&#39;Pos&#39;].eq(&#39;SG-PG-SF&#39;))),&#39;Guard&#39;,&#39;FrontCourt&#39;) Tree based models like an XGBoost generally do not do well handling categorical variables. For this reason, I decided to use One-Hot Encoding on our categorical position variable. transformer = make_column_transformer( (OneHotEncoder(sparse=False), [&#39;Pos&#39;]), remainder = &#39;passthrough&#39; ) transformed = transformer.fit_transform(train) train = pd.DataFrame(transformed, columns=transformer.get_feature_names_out()) Test Dataset Now that our training dataset is finalized, we now can create our test dataset with the current season’s data. As always in a modeling project, we must manipulate the test set in the same manner we did our training set. #Test set read in test = pd.read_csv(&#39;C:/Users/ericd/OneDrive - North Carolina State University/Desktop/NBA-AllStar-Predictions/Data/season22-23.csv&#39;) Similar to with the training set, I created a new function to apply all manipulations to the test set at once, including the scaling, position variable cleaning, and One-Hot Encoding. #Test set cleaning function def testPrep(df): df = df.sort_values([&#39;Player&#39;,&#39;G&#39;], ascending = [True, False]) df = df.drop_duplicates(subset=[&#39;Player&#39;], keep=&#39;first&#39;) df = df[df[&#39;MP&#39;] &gt;= 15] df = df.drop([&#39;Rk&#39;,&#39;Tm&#39;], axis=1) df[&#39;Age&#39;] = zscore(df[&#39;Age&#39;]) df[&#39;G&#39;] = zscore(df[&#39;G&#39;]) df[&#39;GS&#39;] = zscore(df[&#39;GS&#39;]) df[&#39;MP&#39;] = zscore(df[&#39;MP&#39;]) df[&#39;FG&#39;] = zscore(df[&#39;FG&#39;]) df[&#39;FGA&#39;] = zscore(df[&#39;FGA&#39;]) df[&#39;FG%&#39;] = zscore(df[&#39;FG%&#39;]) df[&#39;3P&#39;] = zscore(df[&#39;3P&#39;]) df[&#39;3PA&#39;] = zscore(df[&#39;3PA&#39;]) df[&#39;2P&#39;] = zscore(df[&#39;2P&#39;]) df[&#39;2PA&#39;] = zscore(df[&#39;2PA&#39;]) df[&#39;2P%&#39;] = zscore(df[&#39;2P%&#39;]) df[&#39;eFG%&#39;] = zscore(df[&#39;eFG%&#39;]) df[&#39;FT&#39;] = zscore(df[&#39;FT&#39;]) df[&#39;FTA&#39;] = zscore(df[&#39;FTA&#39;]) df[&#39;FT%&#39;] = zscore(df[&#39;FT%&#39;]) df[&#39;ORB&#39;] = zscore(df[&#39;ORB&#39;]) df[&#39;DRB&#39;] = zscore(df[&#39;DRB&#39;]) df[&#39;TRB&#39;] = zscore(df[&#39;TRB&#39;]) df[&#39;AST&#39;] = zscore(df[&#39;AST&#39;]) df[&#39;STL&#39;] = zscore(df[&#39;STL&#39;]) df[&#39;BLK&#39;] = zscore(df[&#39;BLK&#39;]) df[&#39;TOV&#39;] = zscore(df[&#39;TOV&#39;]) df[&#39;PF&#39;] = zscore(df[&#39;PF&#39;]) df[&#39;PTS&#39;] = zscore(df[&#39;PTS&#39;]) df[&#39;Pos&#39;] = np.where(((df[&#39;Pos&#39;].eq(&#39;PG&#39;)) | (df[&#39;Pos&#39;].eq(&#39;SG&#39;))),&#39;Guard&#39;,&#39;Frontcourt&#39;) #one hot encoding for test transformer = make_column_transformer( (OneHotEncoder(sparse=False), [&#39;Pos&#39;]), remainder = &#39;passthrough&#39; ) transformed = transformer.fit_transform(df) df = pd.DataFrame(transformed, columns=transformer.get_feature_names_out()) return df I then ran the test set through the function defined above. #Send test set through cleaning function test = testPrep(test) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
